{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "---\n",
    "\n",
    "Authors: Tina Pai, Ravindra  Thanniru, Walter Lai, and Jamie Vo\n",
    "\n",
    "\n",
    "### General Questions:\n",
    "1. Are the model comparisons accurate when different methods are used for parameter tuning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data/case_8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>...</th>\n",
       "      <th>v122</th>\n",
       "      <th>v123</th>\n",
       "      <th>v124</th>\n",
       "      <th>v125</th>\n",
       "      <th>v126</th>\n",
       "      <th>v127</th>\n",
       "      <th>v128</th>\n",
       "      <th>v129</th>\n",
       "      <th>v130</th>\n",
       "      <th>v131</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.335739</td>\n",
       "      <td>8.727474</td>\n",
       "      <td>C</td>\n",
       "      <td>3.921026</td>\n",
       "      <td>7.915266</td>\n",
       "      <td>2.599278</td>\n",
       "      <td>3.176895</td>\n",
       "      <td>0.012941</td>\n",
       "      <td>...</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.989780</td>\n",
       "      <td>0.035754</td>\n",
       "      <td>AU</td>\n",
       "      <td>1.804126</td>\n",
       "      <td>3.113719</td>\n",
       "      <td>2.024285</td>\n",
       "      <td>0</td>\n",
       "      <td>0.636365</td>\n",
       "      <td>2.857144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.630686</td>\n",
       "      <td>7.464411</td>\n",
       "      <td>C</td>\n",
       "      <td>4.145098</td>\n",
       "      <td>9.191265</td>\n",
       "      <td>2.436402</td>\n",
       "      <td>2.483921</td>\n",
       "      <td>2.301630</td>\n",
       "      <td>...</td>\n",
       "      <td>6.822439</td>\n",
       "      <td>3.549938</td>\n",
       "      <td>0.598896</td>\n",
       "      <td>AF</td>\n",
       "      <td>1.672658</td>\n",
       "      <td>3.239542</td>\n",
       "      <td>1.957825</td>\n",
       "      <td>0</td>\n",
       "      <td>1.925763</td>\n",
       "      <td>1.739389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  target        v1        v2 v3        v4        v5        v6        v7  \\\n",
       "0   3       1  1.335739  8.727474  C  3.921026  7.915266  2.599278  3.176895   \n",
       "1   4       1  1.630686  7.464411  C  4.145098  9.191265  2.436402  2.483921   \n",
       "\n",
       "         v8  ...      v122      v123      v124  v125      v126      v127  \\\n",
       "0  0.012941  ...  8.000000  1.989780  0.035754    AU  1.804126  3.113719   \n",
       "1  2.301630  ...  6.822439  3.549938  0.598896    AF  1.672658  3.239542   \n",
       "\n",
       "       v128  v129      v130      v131  \n",
       "0  2.024285     0  0.636365  2.857144  \n",
       "1  1.957825     0  1.925763  1.739389  \n",
       "\n",
       "[2 rows x 133 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning\n",
    "---\n",
    "\n",
    "[Random Search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) is used opposed to a grid search due to the lower computational time required. While a grid search is more thorough, a randomized search will result in comparable metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extreme Gradient Boost XGBoost\n",
    "---\n",
    "[xbgboost.cv](https://xgboost.readthedocs.io/en/latest/python/python_api.html) is the cross validation method used opposed to K-fold.\n",
    "\n",
    "\n",
    "XGBoost is an algorithm which takes a collection of weak learners and create a strong learner. Weak learners are considered to be only slightly better than a random guess. Through each round, the model attemps to improve the prediction compared to the previous. At a high level, xgboost fits a model, determines the error rates, and then fits a new model using the error rates as the new target. In doing so, every round has a new error rate target until the residuals are randomized.  \n",
    "\n",
    "Due to the nature of xgboost, any model can be fitted using the method since the error rates are used as the targets. Weak learners are created by predicting on a small portion of the data opposed to all of the unknowns. \n",
    "\n",
    "Xgboost models are strongly suseptible to overfitting. In order to prevent this, the model is defaulted to regularization. \n",
    "\n",
    "\n",
    "#### Loss Calculation\n",
    "XGBoost uses an approximate loss calculation. So long as a loss function has a first and second order partial derivative, xgboost is applicable. \n",
    "<img src=\"../Images/Loss_formula.png\" alt=\"Loss Formula\" style=\"width: 400px;\"/>\n",
    "<img src=\"../Images/Penalty_formula.png\" alt=\"Penalty Formula\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parameters for XGBoost\n",
    "\n",
    "The bolded feature are of highest significance when tuning the model. \n",
    "\n",
    "---\n",
    "\"objective\": \"binary:logistic\" - since the dataset is binary, the binary logistic is used <br>\n",
    "\"booster\": \"gbtree\" <br>\n",
    "\"eval_metric\": \"logloss\" - logloss is selected as the evaluation metric for consideration of the best model <br>\n",
    "**\"eta\"**: 0.01 - learning rate, generally lower is better, but be cautious of a learning rate so slow that the model never stops <br> \n",
    "<br>Since XGBoost is a partition tree, the parameters below are relatable to tree parameters. <br><br>\n",
    "**\"subsample\"**:  - generally closer to 1, this is similar to bagging and prevents over-sampling (row-level)<br> \n",
    "**\"colsample_bytree\"**: - generally closer to 1, this is the same as subsampling, but for columns<br> \n",
    "**\"colsample_bylevel\"**: sub-sampling of levels <br>\n",
    "**\"colsample_bynode\"**: sub-sampling of nodes <br>\n",
    "**\"max_depth\"**: the depth of the tree - generally a good rule of thumb is the squareroot of the number of features<br>\n",
    "\"boosting_round\":  a higher number of rounds is recommended, once the loss no longer reduces, kill the model <br>\n",
    "\"gamma\": penalty for more nodes/leafs <br>\n",
    "**\"max_child_weight\"**: the weight to exceed for partition creation <br>\n",
    "\"lambda\": L2 regularization - leave at standard (ON) <br>\n",
    "\"alpha\": L1 regularization - leave at standard (OFF)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data prep for XGBoost\n",
    "xg = xgb.DMatrix(X.values, y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost params:\n",
    "xgboost_params = { \n",
    "   \"objective\": \"binary:logistic\", # do not change\n",
    "   \"booster\": \"gbtree\", # do not change\n",
    "   \"eval_metric\": \"logloss\", # do not change\n",
    "   \"eta\": 0.01, \n",
    "   \"subsample\": 0.5,\n",
    "   \"colsample_bytree\": 0.5,\n",
    "   \"max_depth\": 3\n",
    "}\n",
    "boost_round = 50 # recommended to be near 400-500 rounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation\n",
    "xgboost.cv(xgboost_params,\n",
    " dtrain, # set this to the training matrix***\n",
    " num_boost_round=1000,\n",
    " nfold=3, # set to the number required\n",
    " stratified=TRUE, # stratify TRUE\n",
    " metrics=(\"accuracy\", \"logloss\"), # metrics desired\n",
    " obj=None, # this is the binary logloss***\n",
    " early_stopping_rounds=None, # when to stop the rounds*** recommended ~200/300. This needs to be triggered, if it doesn't it means there is an error\n",
    " seed=123,\n",
    " shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "clf = xgb.train(xgboost_params,xg,num_boost_round=boost_round,verbose_eval=True,maximize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "---\n",
    "\n",
    "Random forest or SVM will have the lowest score; XGBoost will have a significantly higher score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=50) # n_estimators of 50 is too low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)\n",
    "Linear SVM is used for the model analysis.\n",
    "\n",
    "Test/train split is applicable in this situation due to the high computation time of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Metrics\n",
    "---\n",
    "\n",
    "SVM does not have a log-loss score due to its requirements of a probability prediction, which SVM is not capable of providing. While packages can be added on after running the SVM model, due to the requirement of cross-validation, the computation time required is not applicable for this study. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Scaling\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MSDS ML I",
   "language": "python",
   "name": "ml_i_msds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
